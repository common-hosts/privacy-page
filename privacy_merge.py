import html
import sys
import urllib
import subprocess
import os

import requests
import time
from bs4 import BeautifulSoup

import base64
import gzip
import json
import re
from io import BytesIO
from pathlib import Path
from DrissionPage import Chromium

table_url = "https://superxgr.larksuite.com/base/SebGbrq2yaNXXSsVOcJudpzxsCf?table=tblTywpT1yCgOaV7&view=vewOnkM00z"
api_keyword = "SebGbrq2yaNXXSsVOcJudpzxsCf/records"
browser = None
browser_port = 9527
cookies_str = ""
app_name: str = ""
company_name: str = ""
email: str = ""

# ç”¨äº finally å®‰å…¨é€€å‡º
driver = None

# ç”Ÿæˆå¹¶å‘å¸ƒé™æ€é¡µéœ€è¦çš„è¾“å‡ºæ–‡ä»¶
PRIVACY_TEXT_OUT = Path(__file__).resolve().parent / "privacy_text.txt"

# muban.html æ¨¡æ¿è·¯å¾„ï¼ˆå†…å®¹å›ºå®šï¼Œåªæ›¿æ¢å°‘é‡å­—æ®µï¼‰
MUBAN_TEMPLATE_PATH = Path(__file__).resolve().parent / "muban.html"


def html_to_formatted_text(html_fragment: str) -> str:
    """å°† privacy_simple_content çš„ innerHTML è½¬æˆè¾ƒå¥½ç²˜è´´çš„çº¯æ–‡æœ¬ï¼Œä¿ç•™æ®µè½ã€åˆ—è¡¨å’Œé“¾æ¥ç»“æ„ã€‚"""
    if not html_fragment:
        return ""
    soup = BeautifulSoup(html_fragment, "html.parser")

    lines = []

    from bs4.element import NavigableString, Tag

    def handle_node(node, indent_level=0):
        indent = "  " * indent_level

        if isinstance(node, NavigableString):
            text = str(node)
            text = text.replace("\u200b", "").strip("\n")
            if text:
                lines.append(indent + text)
            return

        if not isinstance(node, Tag):
            return

        name = (node.name or "").lower()

        if name == "br":
            lines.append("")
            return

        if name in {"p", "div", "section", "strong", "b", "em", "i", "h1", "h2", "h3", "h4", "h5", "h6"}:
            before = len(lines)
            for child in node.children:
                handle_node(child, indent_level)
            after = len(lines)
            if after > before and (not lines or lines[-1] != ""):
                lines.append("")
            return

        if name in {"ul", "ol"}:
            if lines and lines[-1] != "":
                lines.append("")
            idx = 1
            for li in node.find_all("li", recursive=False):
                prefix = "- " if name == "ul" else f"{idx}. "
                buf = []

                def collect(child):
                    if isinstance(child, NavigableString):
                        t = str(child).replace("\u200b", "").strip("\n")
                        if t:
                            buf.append(t)
                    elif isinstance(child, Tag):
                        cname = (child.name or "").lower()
                        if cname == "br":
                            buf.append(" ")
                        elif cname == "a":
                            href = child.get("href") or ""
                            visible = child.get_text(strip=True)
                            if href and visible:
                                buf.append(f"{visible} ({href})")
                            else:
                                buf.append(visible or href)
                        else:
                            for g in child.children:
                                collect(g)

                for c in li.children:
                    collect(c)
                li_text = "".join(buf)
                li_text = re.sub(r"\s+", " ", li_text).strip()
                if li_text:
                    lines.append(indent + prefix + li_text)
                for sub in li.find_all(["ul", "ol"], recursive=False):
                    handle_node(sub, indent_level + 1)
                if lines and lines[-1] != "":
                    lines.append("")
                idx += 1
            return

        if name == "a":
            href = node.get("href") or ""
            visible = node.get_text(strip=True)
            if href and visible:
                lines.append(indent + f"{visible} ({href})")
            else:
                lines.append(indent + (visible or href))
            return

        for child in node.children:
            handle_node(child, indent_level)

    root = soup.find(id="privacy_simple_content") or soup
    for c in root.children:
        handle_node(c, 0)

    out = []
    blank = 0
    for ln in lines:
        if ln.strip() == "":
            blank += 1
            if blank <= 2:
                out.append("")
        else:
            blank = 0
            out.append(ln)

    text = "\n".join(out)
    text = text.replace("\r\n", "\n")
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text


def get_gzip_json_from_api(timeout: int = 60):
    """
    1. ç›‘å¬æ¥å£æ•è·åŠ¨æ€å‚æ•°ã€‚
    2. æ‰‹åŠ¨æ‰«ç ç™»å½•ï¼Œåˆ·æ–°é¡µé¢è§¦å‘æ¥å£ã€‚
    3. ä¿®æ”¹æ•è·åˆ°çš„ URLï¼Œè®¾ç½® offset=0ã€‚
    4. æå– Cookiesï¼Œä½¿ç”¨ requests åº“é‡æ–°å‘é€è¯·æ±‚ã€‚
    5. è§£æå“åº”ï¼Œè§£å‹ Gzip æ•°æ®ã€‚
    """
    global browser
    if browser is None:
        browser = Chromium(browser_port)

    tab = browser.latest_tab
    tab.get(table_url)
    print(f"ğŸ” å¼€å§‹ç›‘å¬æ¥å£: {api_keyword}")
    tab.listen.start(api_keyword)

    input("è¯·æ‰«ç ç™»å½•å¹¶æŒ‰ Enter ç»§ç»­ >>> ")
    tab.refresh()  # è§¦å‘æ¥å£è¯·æ±‚

    print(f"ğŸ” å¼€å§‹æ•è·æ¥å£è¯·æ±‚...")
    # ç­‰å¾…æ¥å£è§¦å‘
    req = tab.listen.wait(timeout=timeout)
    tab.listen.stop()  # æ•è·åˆ°ååœæ­¢ç›‘å¬

    if not req:
        print(f"âŒ {timeout} ç§’å†…æœªæ•è·åˆ°æ¥å£è¯·æ±‚ã€‚")
        return None

    # --- 1. è·å–åŸå§‹ URL å¹¶ä¿®æ”¹ offset ---
    original_url = req.url
    print(f"âœ… æ•è·åˆ°åŸå§‹æ¥å£: {original_url}")

    # è§£æ URL å’ŒæŸ¥è¯¢å‚æ•°
    parsed_url = urllib.parse.urlparse(original_url)
    query_params = urllib.parse.parse_qs(parsed_url.query)

    # ä¿®æ”¹ offset å‚æ•°ä¸º 0ï¼ˆè·å–æ‰€æœ‰æ•°æ®ï¼‰
    query_params["offset"] = ["0"]

    # é‡æ–°æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²å’Œå®Œæ•´çš„ URL
    new_query_string = urllib.parse.urlencode(query_params, doseq=True)
    new_url = urllib.parse.urlunparse(parsed_url._replace(query=new_query_string))

    print(f"ğŸ”„ æ­£åœ¨ç”¨ä¿®æ”¹åçš„ URL (åå°è¯·æ±‚): {new_url}")

    # --- 2. æå–å·²ç™»å½•çš„ Cookies ---
    current_cookies = tab.cookies()

    # å°† cookies è½¬æ¢ä¸ºå­—ç¬¦ä¸²å½¢å¼ï¼Œä½œä¸º HTTP è¯·æ±‚çš„å¤´éƒ¨
    cookies_str = "; ".join(
        [f"{cookie['name']}={cookie['value']}" for cookie in current_cookies]
    )

    # è®¾ç½® headersï¼Œå¸¦ä¸Š cookies
    headers = {"Cookie": cookies_str}

    # --- 3. ä½¿ç”¨ requests å‘é€è¯·æ±‚ ---
    try:
        response = requests.get(new_url, headers=headers, timeout=timeout)
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None

    # --- 4. æ£€æŸ¥å’Œæå–å“åº”ä½“ ---

    if response.status_code != 200:
        print(f"âŒ é‡æ–°è¯·æ±‚å¤±è´¥ï¼HTTP çŠ¶æ€ç : {response.status_code}")
        return None

    resp_body_text = response.text

    if not resp_body_text:
        print(f"âŒ é‡æ–°è¯·æ±‚æˆåŠŸ (200)ï¼Œä½†å“åº”ä½“ä¸ºç©ºã€‚")
        return None

    # --- 5. è§£æ JSON å’Œ Gzip è§£å‹ ---

    try:
        resp_json = json.loads(resp_body_text)
    except Exception as e:
        print(f"âš ï¸ å“åº”ä¸æ˜¯åˆæ³• JSONï¼š{e}\nåŸå§‹å†…å®¹: {resp_body_text[:200]}")
        return None

    # æå– gzip Base64 æ•°æ®
    try:
        gzip_base64_str = resp_json["data"]["records"]
    except KeyError:
        print("âŒ æœªæ‰¾åˆ° data.records å­—æ®µï¼Œè¯·æ£€æŸ¥è¿”å›ç»“æ„ã€‚")
        return None

    try:
        gzip_bytes = base64.b64decode(gzip_base64_str)
        with gzip.GzipFile(fileobj=BytesIO(gzip_bytes)) as f:
            decompressed_data = f.read().decode("utf-8")
        records_json = json.loads(decompressed_data)
    except Exception as e:
        print(f"âŒ è§£å‹æˆ–è§£æå¤±è´¥: {e}")
        return None

    print("âœ… æˆåŠŸè§£å‹ JSON æ•°æ®ï¼")
    return records_json, cookies_str


try:
    from selenium.webdriver.support import expected_conditions as EC
except Exception:
    EC = None


def normalize_text(s):
    if not s:
        return ""
    return s.replace('\u200b', '').strip()


# ï¼ˆå·²åœç”¨ï¼‰ä»¥ä¸‹ Selenium ç›¸å…³é€»è¾‘ä¸ºæ—§ç‰ˆéšç§ç½‘ç«™è‡ªåŠ¨åŒ–æµç¨‹ï¼Œå½“å‰æ¨¡æ¿æ–¹æ¡ˆä¸å†éœ€è¦ã€‚
# ä¸ºé¿å…è¿è¡ŒæœŸè¯¯è§¦å‘æ‰“å¼€/å…³é—­æµè§ˆå™¨ï¼Œè¿™é‡Œç§»é™¤ç›¸å…³å‡½æ•°å…¥å£ã€‚
# - ensure_check_checkbox
# - click_next_footer
# - _close_modal_if_possible
# - extract_and_show_privacy_text
# - create_driver


# 
# GitHub Pages / SSH helpers
# 


def _decode_bytes(b: bytes | None) -> str:
    """Decode subprocess output safely (avoid Windows GBK crashes)."""
    if not b:
        return ""
    try:
        return b.decode("utf-8", errors="replace")
    except Exception:
        return b.decode(errors="replace")


def _run_capture(cmd: list[str], *, env: dict | None = None, cwd: str | None = None) -> tuple[int, str, str]:
    """Run a command and capture stdout/stderr safely (bytes -> utf-8 replace)."""
    p = subprocess.run(cmd, env=env, cwd=cwd, capture_output=True)
    return p.returncode, _decode_bytes(p.stdout), _decode_bytes(p.stderr)


def ensure_github_ssh_keychain_ready(key_path: str = "~/.ssh/id_ed25519_common_hosts") -> None:
    """Teammate-friendly: don't spam `ssh-add` output and don't block on passphrase.

    We only *check* whether key is loaded. If not loaded, we print a one-time hint.
    Loading should be done manually once:
      ssh-add --apple-use-keychain ~/.ssh/id_ed25519_common_hosts
    """

    def _pub_key_body(pub_text: str) -> str:
        parts = (pub_text or "").strip().split()
        return parts[1] if len(parts) >= 2 else ""

    try:
        kp = Path(key_path).expanduser()
        if not kp.exists():
            return

        pub_path = Path(str(kp) + ".pub")
        if not pub_path.exists():
            return

        want_body = _pub_key_body(pub_path.read_text(encoding="utf-8"))
        if not want_body:
            return

        ret, out, _err = _run_capture(["ssh-add", "-L"])
        if ret == 0 and want_body in (out or ""):
            return

        print(
            "\nâš ï¸ æ£€æµ‹åˆ° GitHub Pages çš„ SSH key è¿˜æœªåŠ è½½åˆ° ssh-agentï¼ˆæˆ–æœªä¿å­˜åˆ° Keychainï¼‰ã€‚\n"
            "è¯·åœ¨ç»ˆç«¯æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰ï¼š\n"
            f"  ssh-add --apple-use-keychain {kp}\n"
            "è¾“å…¥ passphrase åï¼Œä»¥åè„šæœ¬è¿è¡Œå°±ä¸ä¼šå†æç¤ºã€‚\n"
        )
    except Exception:
        pass


def _run_git_push_main_with_env() -> None:
    """Best-effort fallback push using the preferred SSH host alias.

    Note: We do NOT pass '-i key' here to avoid interactive passphrase prompts.
    Use ssh-agent+Keychain for non-interactive use.
    """
    env = os.environ.copy()
    env.setdefault("PRIVACY_PAGES_SSH_HOST", "github-common-hosts")

    env["GIT_SSH_COMMAND"] = (
        "ssh -o BatchMode=yes -o IdentitiesOnly=yes "
        "-o StrictHostKeyChecking=accept-new "
        "-o ControlMaster=auto -o ControlPersist=10m -o ControlPath=~/.ssh/cm-%r@%h:%p"
    )

    repo_root = Path(__file__).resolve().parent

    # push regardless of status (commit may already exist)
    subprocess.run(["git", "push", "origin", "main"], cwd=str(repo_root), env=env, check=False)


def publish_privacy_page_to_github(app_title: str, publish_id: str, content_file: Path) -> str:
    """Call googleSites.py to generate & push pages/<slug>/index.html.

    Return: published page URL (best-effort parsed).
    """
    env = os.environ.copy()
    env.setdefault("PRIVACY_PAGES_SSH_HOST", "github-common-hosts")
    env.setdefault("PRIVACY_PAGES_SSH_KEY", str(Path("~/.ssh/id_ed25519_common_hosts").expanduser()))

    safe_title = (app_title or "privacy-policy").strip() or "privacy-policy"
    safe_id = (publish_id or "").strip()

    cmd = [
        sys.executable,
        str(Path(__file__).resolve().parent / "googleSites.py"),
        "--title",
        safe_title,
        "--id",
        safe_id,
        "--content-file",
        str(content_file),
        "--commit-message",
        f"Publish privacy page: {safe_title}",
        "--no-wait",
    ]

    rc, stdout, stderr = _run_capture(cmd, env=env)
    combined = (stdout or "") + ("\n" + (stderr or "") if stderr else "")

    if combined.strip():
        print("------ googleSites.py è¾“å‡ºå¼€å§‹ ------")
        print(combined.strip())
        print("------ googleSites.py è¾“å‡ºç»“æŸ ------")

    m = re.search(r"(https?://[^\s]+/pages/[^\s]+/)", combined)
    page_url = m.group(1) if m else ""

    if rc != 0:
        print("âš ï¸ googleSites.py è¿”å›é 0ï¼Œå°è¯•å…œåº• push ä¸€æ¬¡...")
        try:
            _run_git_push_main_with_env()
        except Exception:
            pass

    return page_url


def build_privacy_html_from_template(app_name_value: str, company_name_value: str, email_value: str) -> str:
    """åŸºäº muban.html æ›¿æ¢å…³é”®å­—æ®µç”Ÿæˆæœ€ç»ˆ HTMLã€‚

    åªåš 2 å¤„æ›¿æ¢ï¼š
      1) "This privacy policy applies to the <APP> app ... created by <COMPANY> ..."
      2) "please contact the Service Provider via email at <EMAIL>."

    æ¨¡æ¿å…¶ä»–ä¿æŒä¸å˜ã€‚
    """
    tpl = MUBAN_TEMPLATE_PATH.read_text(encoding="utf-8")

    app_safe = (app_name_value or "").strip()
    company_safe = (company_name_value or "").strip()
    email_safe = (email_value or "").strip()

    if not app_safe or not company_safe or not email_safe:
        print(f"âš ï¸ æ¨¡æ¿æ›¿æ¢å­—æ®µå¯èƒ½ä¸ºç©º: app_name={app_safe!r}, company_name={company_safe!r}, email={email_safe!r}")

    # 1) æ›¿æ¢ app_name / company_nameï¼ˆåªæ›¿æ¢è¿™ä¸€å¥é‡Œçš„éƒ¨åˆ†ï¼‰
    #    ç”¨éè´ªå©ªåŒ¹é…ï¼Œå°½é‡ä¸ç ´åæ¨¡æ¿å…¶å®ƒå†…å®¹ã€‚
    def _repl_main(m: re.Match) -> str:
        return (
            "This privacy policy applies to the "
            + app_safe
            + " app"
            + m.group(1)
            + "created by "
            + company_safe
            + m.group(2)
        )

    main_pat = re.compile(
        r"This privacy policy applies to the\s+.*?\s+app(\s*\(hereby referred to as\s+&quot;Application&quot;\)\s+for mobile devices that was\s+)(?:created by\s+).*?(\s+\(hereby referred to as\s+&quot;Service Provider&quot;\)\s+as a Free service)",
        re.I | re.S,
    )
    new_tpl, n1 = main_pat.subn(_repl_main, tpl, count=1)
    if n1 == 0:
        # å…œåº•ï¼šå¦‚æœæ¨¡æ¿å¥å­ç•¥æœ‰ä¸åŒï¼Œå°è¯•å®½æ¾ä¸€ç‚¹çš„åŒ¹é…
        loose_pat = re.compile(r"This privacy policy applies to the\s+.*?\s+app\s*\(.*?\)\s+for mobile devices that was created by\s+.*?\s*\(.*?\)\s+as a Free service", re.I | re.S)
        loose_match = loose_pat.search(new_tpl)
        if loose_match:
            s = loose_match.group(0)
            s2 = re.sub(r"This privacy policy applies to the\s+.*?\s+app", f"This privacy policy applies to the {app_safe} app", s, flags=re.I | re.S)
            s2 = re.sub(r"created by\s+.*?\s*\(", f"created by {company_safe} (", s2, flags=re.I | re.S)
            new_tpl = new_tpl.replace(s, s2)
            n1 = 1

    # 2) æ›¿æ¢åº•éƒ¨ Contact Us é‡Œçš„é‚®ç®±ï¼ˆå¯èƒ½å‡ºç°å¤šæ¬¡ï¼Œæˆ‘ä»¬æ›¿æ¢å…¨éƒ¨ï¼‰
    #    æŒ‰ç”¨æˆ·è¯´çš„é‚£å¥æ¥æ›¿æ¢ï¼ˆä¸æ”¹å˜å…¶å®ƒåœ°æ–¹ï¼‰
    contact_pat = re.compile(
        r"please contact the Service Provider via email at\s+[^<\s]+@gmail\.com\.",
        re.I,
    )
    new_tpl2, n2 = contact_pat.subn(
        f"please contact the Service Provider via email at {email_safe}.",
        new_tpl,
    )

    # æ¨¡æ¿é‡Œä¹Ÿå¯èƒ½æœ‰æ‹¬å·å½¢å¼çš„é‚®ç®±ï¼ˆä¾‹å¦‚ Children æ®µï¼‰ï¼Œä¸€å¹¶æ›¿æ¢åŒä¸€ä¸ªé‚®ç®±
    new_tpl3 = re.sub(r"\([^\s()]+@gmail\.com\)", f"({email_safe})", new_tpl2, flags=re.I)

    if n1 == 0:
        print("âš ï¸ æœªå‘½ä¸­æ¨¡æ¿ä¸»å¥æ›¿æ¢ï¼ˆapp_name/company_nameï¼‰ï¼Œè¯·ç¡®è®¤ muban.html ä¸­è¯¥å¥æ˜¯å¦æœ‰æ”¹åŠ¨ã€‚")
    if n2 == 0:
        print("âš ï¸ æœªå‘½ä¸­æ¨¡æ¿ Contact Us é‚®ç®±æ›¿æ¢ï¼ˆemailï¼‰ï¼Œè¯·ç¡®è®¤ muban.html ä¸­è¯¥å¥æ˜¯å¦æœ‰æ”¹åŠ¨ã€‚")

    return new_tpl3


def privacy_html_to_plain_text(html_doc: str) -> str:
    """æŠŠæ¨¡æ¿ HTML è½¬æˆæ›´é€‚åˆç²˜è´´çš„çº¯æ–‡æœ¬ï¼Œä¿ç•™æ¢è¡Œ/åˆ—è¡¨/é“¾æ¥ã€‚"""
    soup = BeautifulSoup(html_doc or "", "html.parser")
    content = soup.select_one("#privacy_simple_content")
    # æˆ‘ä»¬çš„ muban.html ä¸ä¸€å®šæœ‰è¿™ä¸ª idï¼Œè¿™é‡Œå…¼å®¹ï¼šä¼˜å…ˆå– .content
    if content is None:
        content = soup.select_one(".content")
    if content is None:
        content = soup

    # ä½¿ç”¨å·²æœ‰çš„ html_to_formatted_textï¼šå®ƒæ¥å— innerHTML
    return html_to_formatted_text(str(content))


def generate_privacy_text_from_muban() -> str:
    """ç›´æ¥ç”¨ muban.html ç”Ÿæˆéšç§æ–‡æœ¬ï¼ˆæ— éœ€æ‰“å¼€éšç§ç”Ÿæˆç½‘ç«™ï¼‰ã€‚"""
    html_doc = build_privacy_html_from_template(app_name, company_name, email)
    text = privacy_html_to_plain_text(html_doc)
    if not text:
        raise RuntimeError("æœªèƒ½ä» muban.html ç”Ÿæˆå¯ç”¨çš„éšç§æ–‡æœ¬")

    # å†™æ–‡ä»¶ä¾›å‘å¸ƒè„šæœ¬ä½¿ç”¨
    PRIVACY_TEXT_OUT.write_text(text, encoding="utf-8")
    return text


# python
def run_privacy_flow(publish_id: str = ""):
    """ç”Ÿæˆéšç§æ–‡æœ¬æ–‡ä»¶å¹¶å‘å¸ƒåˆ° GitHub Pagesã€‚

    æ³¨æ„ï¼šæ­¤æµç¨‹ä¸å†æ‰“å¼€ Selenium æµè§ˆå™¨ã€‚
    æµè§ˆå™¨ä»…ç”¨äº get_gzip_json_from_api() çš„ Lark ç™»å½•/æŠ“å–ã€‚
    """

    # 1) ç”¨æ¨¡æ¿ç”Ÿæˆéšç§æ–‡æœ¬ï¼ˆå†™å…¥ privacy_text.txtï¼‰
    _ = generate_privacy_text_from_muban()

    # 2) å‘å¸ƒåˆ° GitHub Pages
    print("ğŸš€ ç½‘é¡µå‘å¸ƒä¸­ã€‚ã€‚ã€‚")
    page_url = publish_privacy_page_to_github(
        app_title=(app_name or "privacy-policy"),
        publish_id=publish_id,
        content_file=PRIVACY_TEXT_OUT,
    )

    if page_url:
        print(f"ğŸŒ å·²å‘å¸ƒç½‘é¡µåœ°å€: {page_url}")

    return True


def find_and_collect_by_target_value(json_obj, target_value=None):
    """
    æŒ‰è®¢å•å·ç­›é€‰å¹¶åœ¨æ‰¾åˆ°æ—¶æŠŠ app_name å†™å…¥å…¨å±€å˜é‡ app_nameï¼Œç»§ç»­è¿”å›ç»“æœåˆ—è¡¨ã€‚
    """
    # é¡¶å±‚å‡½æ•°æœ¬èº«ä¸ç›´æ¥è¯»å†™ app_nameï¼Œåªåœ¨å†…éƒ¨åµŒå¥—å‡½æ•°é‡Œæ“ä½œ

    if not target_value:
        print("âŒ éœ€è¦æä¾› target_value (è®¢å•ç¼–å·)ï¼Œä¾‹å¦‚ 'IGT1185'")
        return []

    results = []
    target_str = str(target_value).strip().lower()

    def _search(obj):
        # è¿™é‡Œæ˜¾å¼å£°æ˜ä½¿ç”¨å…¨å±€ app_nameï¼Œé¿å… UnboundLocalError
        global app_name

        if isinstance(obj, dict):
            fld_order = obj.get("fldxQWjXD7")
            if isinstance(fld_order, dict):
                val = fld_order.get("value")
                if isinstance(val, list):
                    for entry in val:
                        if isinstance(entry, dict):
                            text = (entry.get("text") or "").strip()
                            if text.lower() == target_str:
                                # æå– app_nameï¼ˆæ¥è‡ª fldaShB3Gb çš„ç¬¬ä¸€ä¸ª value çš„ textï¼‰
                                found_app_name = None
                                flda = obj.get("fldaShB3Gb")
                                if isinstance(flda, dict):
                                    fval = flda.get("value")
                                    if isinstance(fval, list) and fval:
                                        first = fval[0]
                                        if isinstance(first, dict):
                                            found_app_name = (first.get("text") or "").strip() or None

                                # å°è¯•åœ¨ fldnLglcRi ä¸­å†™å…¥ app_name å¹¶è¿”å›å…¶ç¬¬ä¸€ä¸ª value
                                fldn = obj.get("fldnLglcRi")
                                if isinstance(fldn, dict):
                                    v = fldn.get("value")
                                    if isinstance(v, list) and v:
                                        first_item = v[0]
                                        if isinstance(first_item, dict):
                                            if found_app_name:
                                                first_item["app_name"] = found_app_name
                                            results.append(first_item)
                                        else:
                                            new_item = {"value": first_item}
                                            if found_app_name:
                                                new_item["app_name"] = found_app_name
                                            results.append(new_item)
                                    else:
                                        new_item = {}
                                        if found_app_name:
                                            new_item["app_name"] = found_app_name
                                        results.append(new_item)
                                else:
                                    if found_app_name:
                                        results.append({"app_name": found_app_name})
                                    else:
                                        results.append(obj)

                                # æ— è®ºä¹‹å‰æ˜¯å¦æœ‰å€¼ï¼Œç›´æ¥æŠŠæ‰¾åˆ°çš„ app_name èµ‹ç»™å…¨å±€å˜é‡
                                if found_app_name:
                                    app_name = found_app_name
                                    print(f"ğŸ”§ å·²è®¾ç½®å…¨å±€ app_name = `{app_name}`")

                                break

            # ç»§ç»­é€’å½’æŸ¥æ‰¾å­èŠ‚ç‚¹
            for v in obj.values():
                _search(v)

        elif isinstance(obj, list):
            for item in obj:
                _search(item)

    _search(json_obj)
    return results


def extract_vps_array_from_doc22(doc_data, cookies_str):
    global company_name, email
    print("ğŸ” æå–é¡µé¢ä¸­é¦–ä¸ªæœ‰æ•ˆçš„ @gmail.com é‚®ç®±...")
    results = []
    seen_urls = set()

    headers = {
        "Cookie": cookies_str or "",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0 Safari/537.36",
    }

    email_re = re.compile(r'(?<![A-Za-z0-9._%+\-])([A-Za-z0-9._%+\-]+@gmail\.com)\b', re.I)

    def _clean_gmail_emails(raw_text):
        found = email_re.findall(raw_text or "")
        unique = []
        seen = set()
        for e in found:
            ne = e.strip().lower()
            if ne and ne not in seen:
                seen.add(ne)
                unique.append(ne)
        final = []
        sset = set(unique)
        for e in unique:
            if len(e) > 1 and e[1:] in sset and len(e[0]) == 1:
                continue
            final.append(e)
        return final

    for item in doc_data:
        url = item.get("link")
        text = item.get("text", "")
        if not url and not text:
            continue
        if url in seen_urls:
            continue

        try:
            response = requests.get(url, headers=headers, timeout=15)
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {url}, çŠ¶æ€ç : {response.status_code}")
                continue

            page_content = html.unescape(response.text or "")
            emails = _clean_gmail_emails(page_content)
            primary = emails[0] if emails else ""

            # é¦–æ¬¡å‘ç°æ—¶ï¼Œè®¾ç½®å…¨å±€ company_name å’Œ emailï¼ˆå¦‚æœå°šæœªè®¾ç½®ï¼‰
            if not company_name:
                t = (text or "").strip()
                m = re.search(r'-(.+)$', t)
                if m:
                    company_name = m.group(1).strip()
                else:
                    parts = t.split(None, 1)
                    company_name = parts[1].strip() if len(parts) > 1 else (t or "")

            if primary and not email:
                email = primary.strip().lower()

            results.append(
                {
                    "text": text,
                    "url": url,
                    "email": primary,
                }
            )
            seen_urls.add(url)

        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {url}, é”™è¯¯: {e}")

        # å¦‚æœå…¨å±€ä¿¡æ¯éƒ½å·²å¡«å……ï¼Œå¯ä»¥é€‰æ‹©æå‰é€€å‡ºä»¥åŠ å¿«é€Ÿåº¦
        if company_name and email:
            break

    # æŒ‰ text ä¸­çš„æ•°å­—æ’åºï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
    def _extract_number(t):
        m = re.search(r"(\d+)", (t or ""))
        return int(m.group(1)) if m else 0

    results.sort(key=lambda x: _extract_number(x.get("text")))

    for item in results:
        print(f"{item.get('text')}")
        if item.get('email'):
            print(f"  é‚®ç®±: {item.get('email')}")
        else:
            print("  é‚®ç®±: æœªå‘ç° @gmail.com åœ°å€")
        print("-" * 50)

    print(f"âœ… æ€»æ•°é‡: {len(results)}")
    return results


def save_to_json(data, filename="none.json"):
    Path(filename).write_text(json.dumps(data, ensure_ascii=False, indent=2))
    # print(f"âœ… å·²ä¿å­˜ {len(data)} æ¡ç»“æœåˆ° {filename}")


import argparse


if __name__ == "__main__":
    # åœ¨è¿è¡Œ push ä¹‹å‰åªåšä¸€æ¬¡æ£€æŸ¥ï¼šå¦‚æœ key æ²¡åŠ è½½ï¼Œä¼šæç¤ºåŒäº‹æ‰§è¡Œä¸€æ¬¡ ssh-add
    ensure_github_ssh_keychain_ready()

    parser = argparse.ArgumentParser()
    parser.add_argument('id', nargs='?', help='è¡¨æ ¼ä¸­æŸ¥æ‰¾çš„ç¼–å·ï¼Œä¾‹å¦‚ IGT1128')
    args = parser.parse_args()

    # äº¤äº’è·å– idï¼ˆè‹¥æœªé€šè¿‡å‘½ä»¤è¡Œæä¾›ï¼‰
    if not args.id:
        try:
            args.id = input("è¯·è¾“å…¥ç¼–å·ï¼ˆä¾‹å¦‚ IGT1128ï¼‰ï¼š").strip()
        except (EOFError, KeyboardInterrupt):
            args.id = None

    if not args.id:
        print("âŒ æœªæä¾›ç¼–å·ï¼Œè„šæœ¬å°†é€€å‡ºã€‚\nç¤ºä¾‹ç”¨æ³•ï¼špython privacys.py IGT1128 --scan")
        sys.exit(2)

    try:
        records, cookies_str = get_gzip_json_from_api()
        if not records:
            print("âŒ æœªèƒ½è·å– recordsï¼Œè„šæœ¬é€€å‡º")
            sys.exit(1)

        available_records = find_and_collect_by_target_value(records, target_value=args.id)
        vps_result = extract_vps_array_from_doc22(available_records, cookies_str)

        # ä¸å†åˆ›å»º selenium driverï¼ˆé¿å…è¿è¡ŒæœŸé—´æµè§ˆå™¨å¼¹èµ·åˆå…³é—­ï¼‰
        run_privacy_flow(publish_id=args.id)
    finally:
        # get_gzip_json_from_api ä½¿ç”¨çš„æ˜¯ DrissionPage Chromiumï¼Œä¸æ˜¯ selenium driverï¼›è¿™é‡Œä¸åš driver.quit()
        pass
